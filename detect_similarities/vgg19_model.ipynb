{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vietnamese-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from src.CV_IO_utils import read_imgs_dir\n",
    "from src.CV_transform_utils import apply_transformer\n",
    "from src.CV_transform_utils import resize_img, normalize_img\n",
    "from src.CV_plot_utils import plot_query_retrieval, plot_tsne, plot_reconstructions\n",
    "from src.autoencoder import AutoEncoder\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "serial-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a mode: (autoencoder -> simpleAE, convAE) or (transfer learning -> vgg19)\n",
    "modelName = \"vgg19\"  # also can try: \"simpleAE\", \"convAE\", \"vgg19\"\n",
    "trainModel = True\n",
    "parallel = True  # use multicore processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ignored-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make paths\n",
    "dataTrainDir = os.path.join(os.getcwd(),\"..\", \"notebooks\", \"data_3000\", \"train\",\"happiness\")\n",
    "dataTestDir = os.path.join(os.getcwd(),\"..\", \"notebooks\", \"data_3000\", \"test\",\"happiness\")\n",
    "\n",
    "outDir = os.path.join(os.getcwd(), \"output\", modelName)\n",
    "if not os.path.exists(outDir):\n",
    "    os.makedirs(outDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "strong-beijing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train images from '/home/grace/code/Mac1327/picturing_emotions/detect_similarities/../notebooks/data_3000/train/happiness'...\n",
      "------------------------------------------------------\n",
      "Reading test images from '/home/grace/code/Mac1327/picturing_emotions/detect_similarities/../notebooks/data_3000/test/happiness'...\n",
      "------------------------------------------------------\n",
      "Image shape = (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read images\n",
    "extensions = ['.png']\n",
    "\n",
    "print(\"Reading train images from '{}'...\".format(dataTrainDir))\n",
    "imgs_train = read_imgs_dir(dataTrainDir, extensions, parallel=parallel)\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "\n",
    "print(\"Reading test images from '{}'...\".format(dataTestDir))\n",
    "imgs_test = read_imgs_dir(dataTestDir, extensions, parallel=parallel)\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "\n",
    "shape_img =imgs_train[0].shape\n",
    "\n",
    "print(\"Image shape = {}\".format(shape_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-sacrifice",
   "metadata": {},
   "source": [
    "tem = tf.constant(tf.image.grayscale_to_rgb(np.squeeze(imgs_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "improved-microphone",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VGG19 pre-trained model...\n",
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build models\n",
    "\n",
    "if modelName in [\"simpleAE\", \"convAE\"]:\n",
    "\n",
    "    # Set up autoencoder\n",
    "    info = {\n",
    "        \"shape_img\": shape_img,\n",
    "        \"autoencoderFile\": os.path.join(outDir, \"{}_autoecoder.h5\".format(modelName)),\n",
    "        \"encoderFile\": os.path.join(outDir, \"{}_encoder.h5\".format(modelName)),\n",
    "        \"decoderFile\": os.path.join(outDir, \"{}_decoder.h5\".format(modelName)),\n",
    "    }\n",
    "    model = AutoEncoder(modelName, info)\n",
    "    model.set_arch()\n",
    "\n",
    "    if modelName == \"simpleAE\":\n",
    "        shape_img_resize = shape_img\n",
    "        input_shape_model = (model.encoder.input.shape[1],)\n",
    "        output_shape_model = (model.encoder.output.shape[1],)\n",
    "        n_epochs = 300\n",
    "    elif modelName == \"convAE\":\n",
    "        shape_img_resize = shape_img\n",
    "        input_shape_model = tuple([int(x) for x in model.encoder.input.shape[1:]])\n",
    "        output_shape_model = tuple([int(x) for x in model.encoder.output.shape[1:]])\n",
    "        n_epochs = 500\n",
    "    else:\n",
    "        raise Exception(\"Invalid modelName!\")\n",
    "\n",
    "elif modelName in [\"vgg19\"]:\n",
    "\n",
    "    # Load pre-trained VGG19 model + higher level layers\n",
    "    print(\"Loading VGG19 pre-trained model...\")\n",
    "    model = tf.keras.applications.VGG19(weights='imagenet', include_top=False,\n",
    "                                        input_shape=shape_img)\n",
    "    model.summary()\n",
    "\n",
    "    shape_img_resize = tuple([int(x) for x in model.input.shape[1:]])\n",
    "    input_shape_model = tuple([int(x) for x in model.input.shape[1:]])\n",
    "    output_shape_model = tuple([int(x) for x in model.output.shape[1:]])\n",
    "    n_epochs = None\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Invalid modelName!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beneficial-visitor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape_model = (224, 224, 3)\n",
      "output_shape_model = (7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "# Print some model info\n",
    "print(\"input_shape_model = {}\".format(input_shape_model))\n",
    "print(\"output_shape_model = {}\".format(output_shape_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "processed-conflict",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying image transformer to training images...\n",
      "Applying image transformer to test images...\n"
     ]
    }
   ],
   "source": [
    "# Apply transformations to all images\n",
    "class ImageTransformer(object):\n",
    "\n",
    "    def __init__(self, shape_resize):\n",
    "        self.shape_resize = shape_resize\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img_transformed = resize_img(img, self.shape_resize)\n",
    "        img_transformed = normalize_img(img_transformed)\n",
    "        return img_transformed\n",
    "\n",
    "transformer = ImageTransformer(shape_img_resize)\n",
    "print(\"Applying image transformer to training images...\")\n",
    "imgs_train_transformed = apply_transformer(imgs_train, transformer, parallel=parallel)\n",
    "print(\"Applying image transformer to test images...\")\n",
    "imgs_test_transformed = apply_transformer(imgs_test, transformer, parallel=parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "external-weekend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> X_train.shape = (2700, 224, 224, 3)\n",
      " -> X_test.shape = (150, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Convert images to numpy array\n",
    "X_train = np.array(imgs_train_transformed).reshape((-1,) + input_shape_model)\n",
    "X_test = np.array(imgs_test_transformed).reshape((-1,) + input_shape_model)\n",
    "print(\" -> X_train.shape = {}\".format(X_train.shape))\n",
    "print(\" -> X_test.shape = {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "coupled-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train (if necessary)\n",
    "if modelName in [\"simpleAE\", \"convAE\"]:\n",
    "    if trainModel:\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "        model.fit(X_train, n_epochs=n_epochs, batch_size=256)\n",
    "        model.save_models()\n",
    "    else:\n",
    "        model.load_models(loss=\"binary_crossentropy\", optimizer=\"adam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "graduate-formation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing embeddings using pre-trained model...\n",
      " -> E_train.shape = (2700, 7, 7, 512)\n",
      " -> E_test.shape = (150, 7, 7, 512)\n",
      " -> E_train_flatten.shape = (2700, 25088)\n",
      " -> E_test_flatten.shape = (150, 25088)\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings using model\n",
    "print(\"Inferencing embeddings using pre-trained model...\")\n",
    "E_train = model.predict(X_train)\n",
    "E_train_flatten = E_train.reshape((-1, np.prod(output_shape_model)))\n",
    "E_test = model.predict(X_test)\n",
    "E_test_flatten = E_test.reshape((-1, np.prod(output_shape_model)))\n",
    "print(\" -> E_train.shape = {}\".format(E_train.shape))\n",
    "print(\" -> E_test.shape = {}\".format(E_test.shape))\n",
    "print(\" -> E_train_flatten.shape = {}\".format(E_train_flatten.shape))\n",
    "print(\" -> E_test_flatten.shape = {}\".format(E_test_flatten.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "encouraging-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "if modelName in [\"simpleAE\", \"convAE\"]:\n",
    "    print(\"Visualizing database image reconstructions...\")\n",
    "    imgs_train_reconstruct = model.decoder.predict(E_train)\n",
    "    if modelName == \"simpleAE\":\n",
    "        imgs_train_reconstruct = imgs_train_reconstruct.reshape((-1,) + shape_img_resize)\n",
    "    plot_reconstructions(imgs_train, imgs_train_reconstruct,\n",
    "                         os.path.join(outDir, \"{}_reconstruct.png\".format(modelName)),\n",
    "                         range_imgs=[0, 255],\n",
    "                         range_imgs_reconstruct=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "threaded-relief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting k-nearest-neighbour model on training images...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(metric='cosine')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit kNN model on training images\n",
    "print(\"Fitting k-nearest-neighbour model on training images...\")\n",
    "knn = NearestNeighbors(n_neighbors=5, metric=\"cosine\")\n",
    "knn.fit(E_train_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "entertaining-qualification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing image retrieval on test images...\n"
     ]
    }
   ],
   "source": [
    "# Perform image retrieval on test images\n",
    "print(\"Performing image retrieval on test images...\")\n",
    "for i, emb_flatten in enumerate(E_test_flatten):\n",
    "    _, indices = knn.kneighbors([emb_flatten]) # find k nearest train neighbours\n",
    "    img_query = imgs_test[i] # query image\n",
    "    imgs_retrieval = [imgs_train[idx] for idx in indices.flatten()] # retrieval images\n",
    "    outFile = os.path.join(outDir, \"{}_retrieval_{}.png\".format(modelName, i))\n",
    "    plot_query_retrieval(img_query, imgs_retrieval, outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "induced-timothy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing t-SNE on training images...\n"
     ]
    }
   ],
   "source": [
    "print(\"Visualizing t-SNE on training images...\")\n",
    "outFile = os.path.join(outDir, \"{}_tsne.png\".format(modelName))\n",
    "plot_tsne(E_train_flatten, imgs_train, outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-asbestos",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
